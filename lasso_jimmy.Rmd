---
title: "Logistic-Lasso Coordinate Descent Algorithm"
author: "Jimmy Kelliher (UNI: jmk2303)"
date: "2022-03-20"
output: pdf_document
---

```{r setup, include=FALSE}
source("shared_code/setup.R")
library(glmnet)
```

# Theory

__Lemma 1.__ Consider the optimization problem
  \[ \min_{x \in \mathbb{R}}  \left\{ \frac{1}{2}(x - b)^2 + c|x| \right\} \]
for $b \in\mathbb{R}$ and $c \in \mathbb{R}_{++}$. It follows that the minimizer is given by
  \[ \hat{x} = S(b, c), \]
where $S$ is the soft-thresholding operator.

__Lemma 2.__ Consider the optimization problem
  \[ \min_{\beta_k \in \mathbb{R}} \left\{ \frac{1}{2n} \sum_{i = 1}^n w_i \left(z_i - \sum_{j = 1}^p \beta_j x_{ij} \right)^2 \right\} \]
for some $k \in \{ 1, \ldots, p \}$. It follows that the minimizer is given by
  \[ \hat{\beta}_k = \left( \sum_{i = 1}^n w_i x_{ik}^2 \right)^{-1} \sum_{i = 1}^n w_i x_{ik} \left(z_i - \sum_{j \neq k} \beta_j x_{ij} \right). \]

__Lemma 3.__ With $\hat{\beta}_k$ defined as above,
  \[ \min_{\beta_k \in \mathbb{R}} \left\{ \frac{1}{2n} \sum_{i = 1}^n w_i \left(z_i - \sum_{j = 1}^p \beta_j x_{ij} \right)^2 + \lambda \sum_{j = 1}^p |\beta_j| \right\}
  = \min_{\beta_k \in \mathbb{R}} \left\{ \frac{1}{2}(\beta_k - \hat{\beta}_k)^2 + \left( \frac{1}{n} \sum_{i = 1}^n w_i x_{ik}^2 \right)^{-1} \lambda |\beta_k| \right\}. \] 

__Proposition.__ By Lemma 1 and Lemma 3,
  \[ \underset{\beta_k \in \mathbb{R}}{\arg \min} \left\{ \frac{1}{2n} \sum_{i = 1}^n w_i \left(z_i - \sum_{j = 1}^p \beta_j x_{ij} \right)^2 + \lambda \sum_{j = 1}^p |\beta_j| \right\}
  = S \left(\hat{\beta}_k, \left( \frac{1}{n} \sum_{i = 1}^n w_i x_{ik}^2 \right)^{-1} \lambda \right) \]

\newpage
# Praxis

```{r}
data <-
  read_csv("data/breast-cancer.csv", show_col_types = FALSE) %>%
  mutate(diagnosis = 1 * (diagnosis == "M"))
```

## Helper Functions

```{r}
# logistic function
logistic <- function(x) 1 / (1 + exp(-x))

# shrinkage function
S <- function(beta, gamma) {
  if(abs(beta) <= gamma) {
    0
  } else if(beta > 0) {
    beta - gamma
  } else {
    beta + gamma
  }
}

# probability adjustment function
p_adj <- function(p, epsilon) {
  if (p < epsilon) {
    0
  } else if(p > 1 - epsilon) {
    1
  } else {
    p
  }
}

# weight adjustment function
w_adj <- function(p, epsilon) {
  if ((p < epsilon) | (p > 1 - epsilon)) {
    epsilon
  } else {
    p * (1 - p)
  }
}
```

## Toy Example

```{r}
set.seed(1)
lambda <- 0 #0.0125
epsilon <- 10^(-5)

q    <- 30 - 1
n    <- 1000
X    <- matrix(rnorm(q * n), c(n, q))
X    <- as.matrix(cbind(rep(1, n), X))
y    <- 1 * (runif(n) > 0.5)

# initialize parameters
beta <- rep(0.25, ncol(X))

outer_term <- 0
outer <- 1
while(outer_term < 1) {

p <- map_dbl(logistic(X %*% beta), p_adj, epsilon)
w <- map_dbl(p, w_adj, epsilon)
z <- X %*% beta + (y - p) / w

  terminate <- 0
  iter <- 1
  while(terminate < 1) {
  
    beta_old <- beta
  
    for(k in 1:ncol(X)) {
      x_k    <- X[ , k]
      x_notk <- X[ , -k]
      b_notk <- beta[-k]
  
      # un-penalized coefficient update
      b_k_temp <- sum(w * (z - x_notk %*% b_notk) * x_k) / sum(w * x_k^2)
      # shrinkage update
      b_k      <- S(b_k_temp, lambda * (k > 1) / mean(w * x_k^2))
      # update beta vector along with other parameters
      beta[k] <- b_k
      #p <- map_dbl(logistic(X %*% beta), p_adj, epsilon)
      #w <- map_dbl(p, w_adj, epsilon)
      #z <- X %*% beta + (y - p) / w
    }

    iter <- iter + 1

    if(iter == 100 | max(abs(beta - beta_old)) < 10^(-12)) {
      terminate <- 1
    }

  }

  print(iter)
  outer <- outer + 1

  if(outer == 100 | iter == 2) {
      print(iter)
      outer_term <- 1
  }

}

# true estimates from glmnet
fit <- glmnet(X, y, family = "binomial", standardize = FALSE, lambda = lambda, thresh = 10^-12)

# results
results <- tibble(
    Variable   = 1:length(beta)
  , Jimmy      = beta
  , GLM        = as.vector(glm(y ~ X[ , -1], family = binomial)$coefficients)
  , GLMNET     = as.vector(fit$beta[ , ncol(fit$beta)])
  , Difference = abs(Jimmy - GLMNET)
  , Change     = (Jimmy - GLMNET) / GLMNET
) %>%
  mutate(GLM = na_if(GLM, (lambda != 0) * GLM)) %>%
  filter(Jimmy != 0 | GLMNET != 0)
results %>% knitr::kable()
```

## Test with Actual Data

```{r}
set.seed(1)
epsilon <- 10^(-5)

n    <- nrow(data)
X    <- scale(data[ , -c(1, 2)])
#X    <- data[ , -c(1, 2)]
X    <- as.matrix(cbind(rep(1, n), X))
y    <- data$diagnosis

beta <- rep(0, ncol(X))
lambda_vec <- 1 - log(seq(exp(0.001), exp(1), 0.01))
lambda_vec <- exp(seq(log(0.4), log(0.0004), -0.1))
lambda_vec <- c(0.4, 0.38, 0.36)

for(lambda in lambda_vec) {
# (max(t(X) %*% y) / n)

for(outer in 1:3) {

# initialize parameters
#p <- map_dbl(logistic(X %*% beta), p_adj, epsilon)
#w <- map_dbl(p, w_adj, epsilon)
#z <- X %*% beta + (y - p) / w

terminate <- 0
iter <- 1
while(terminate < 1) {

  p <- map_dbl(logistic(X %*% beta), p_adj, epsilon)
  w <- map_dbl(p, w_adj, epsilon)
  z <- X %*% beta + (y - p) / w

  beta_old <- beta
  # initially go through all parameters
  K <- 1:ncol(X)
  #if(iter > 1) {
  #  K <- which(beta > 0)
  #}

  for(k in K) {
    x_k    <- X[ , k]
    x_notk <- X[ , -k]
    b_notk <- beta[-k]

    # un-penalized coefficient update
    b_k_temp <- sum(w * (z - x_notk %*% b_notk) * x_k) / sum(w * x_k^2)
    # shrinkage update
    b_k      <- S(b_k_temp, lambda * (k > 1) / mean(w * x_k^2))

    # update beta vector along with other parameters
    beta[k] <- b_k
    #p <- map_dbl(logistic(X %*% beta), p_adj, epsilon)
    #w <- map_dbl(p, w_adj, epsilon)
    #z <- X %*% beta + (y - p) / w
  }

  iter <- iter + 1

  if(iter == 1000 | max(abs(beta - beta_old)) < 10^-12) {
    print(iter)
    terminate <- 1
  }

}

}

# True estimates from GLMNET
fit <- glmnet(X, y, family = "binomial", standardize = FALSE, lambda = lambda, thresh = 10^-12)

# results
results <- tibble(
    Variable   = 1:length(beta)
  , Name       = c("intercept", names(data[ , -c(1, 2)]))
  , Jimmy      = beta
  , GLMNET     = as.vector(fit$beta[ , ncol(fit$beta)])
  , Difference = abs(Jimmy - GLMNET)
) %>%
  filter(Jimmy != 0 | GLMNET != 0)

print(paste0("lambda = ", lambda))
print(results %>% knitr::kable())
}
```










## Second Attempt with Actual Data

```{r}
logistic_lasso <- function(
  # a numeric design matrix or data frame with named columns
    inputs
  # a vector of outputs; we must have length(output) == nrow(inputs)
  , output
  # a vector of descending penalization factors, ideally on a logarithmic scale
  , lambda_vec
  # standardize inputs using scale
  , standardize   = TRUE
  # a buffer to prevent divergence when fitted probabilities approach 0 or 1
  , epsilon       = 10^-8
  # maximum number of updates to quadratic approximation of likelihood
  , outer_maxiter =  100
  # maximum number of cycles for coordinate descent given quadratic approximation
  , inner_maxiter = 1000
  # tolerance for convergence of coordinate descent
  , tolerance     = 10^-12
) {

  # standardize data unless otherwise specified
  if(standardize) {

    # format data
    X <- as.matrix(cbind(rep(1, nrow(inputs)), scale(inputs)))
    y <- output  

  } else {

    # format data
    X <- as.matrix(cbind(rep(1, nrow(inputs)), inputs))
    y <- output    

  }

  # initialize coefficients at origin
  beta    <- rep(0, ncol(X))
  beta_df <- NULL

  # begin lambda decrement
  for(lambda in lambda_vec) {

    outer_term <- 0
    outer_iter <- 1

    # update quadratic approximation, execute coordinate descent until convergence, repeat
    while(outer_term < 1) {

      # update quadratic approximation; i.e., taylor expand around current estimates  
      p <- map_dbl(logistic(X %*% beta), p_adj, epsilon)
      w <- map_dbl(p, w_adj, epsilon)
      z <- X %*% beta + (y - p) / w

      inner_term <- 0
      inner_iter <- 1

      # given current quadratic approximation, execute coordinate descent
      while(inner_term < 1) {

        beta_old <- beta

        # execute a complete cycle of coordinate descent
        for(k in 1:ncol(X)) {

          # un-penalized coefficient update
          b_k_temp <- sum(w * (z - X[ , -k] %*% beta[-k]) * X[ , k]) / sum(w * X[ , k]^2)
          # shrinkage update
          b_k      <- S(b_k_temp, (k > 1) * lambda / mean(w * X[ , k]^2))
          # update coefficient vector
          beta[k] <- b_k

        }

        inner_iter <- inner_iter + 1

        if(inner_iter == inner_maxiter | max(abs(beta - beta_old)) < tolerance) {

          inner_term <- 1
      
        }

      }

      outer_iter <- outer_iter + 1

      if(outer_iter == outer_maxiter | inner_iter == 2) {

        outer_term <- 1

      }

    }

    beta_df <- rbind(beta_df, t(beta))

  }

  # format data frame of coefficient estimates
  colnames(beta_df) <- c("intercept", names(inputs))
  beta_df <- as_tibble(beta_df)

  # extract number of variables selected for each lambda
  selected_vec <- apply(beta_df, 1, function(x) sum(x != 0) - 1)

  # output results
  list(lambda = lambda_vec, beta = beta_df, selected = selected_vec)

}
```


```{r}
bad_vars <- c(
    "area_mean", "area_worst", "perimeter_mean", "perimeter_worst", "radius_mean"
  , "perimeter_se", "area_se"
  , "concave points_worst", "concavity_mean"
  , "texture_worst"
)
my_inputs <- data %>% select(-bad_vars) %>% select(-c(1, 2))
my_output <- data$diagnosis

# identify minimum lambda value for which all coefficients are zero
lambda_max <- max(t(scale(as.matrix(my_inputs))) %*% y) / nrow(my_inputs)
# set lambda_min based on scaled data
lambda_min <- 0.0001
# define vector of lambdas
lambda_seq <- exp(seq(log(lambda_max), log(lambda_min), length.out = 100))

output <- logistic_lasso(my_inputs, my_output, lambda_seq, standardize = TRUE)
output
```