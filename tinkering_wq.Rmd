---
title: "Tinkering"
author: "Waveley Qiu (wq2162)"
date: "2022-03-17"
output: pdf_document
---

```{r setup, include=FALSE}
source("shared_code/setup.R")
```

## EDA

Let's import and take a look at the data.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
bc <- read.csv("data/breast-cancer.csv") %>% as_tibble()

# sanity check nrow(bc)
# sanity check ncol(bc)
# sanity check table(bc$diagnosis)
```

Let's take a look at the distributions of other variables.
```{r, echo=FALSE, warning=FALSE, message=FALSE}
bc %>% 
  select(diagnosis, contains("mean")) %>%
  gather(key = "stat", value = "value", 2:11) %>% 
  ggplot(aes(x = value, fill = diagnosis)) +
    facet_wrap(~ stat, scales = "free") +
    geom_density(alpha = 0.5)
```

```{r}
tbl_summary(bc, by = diagnosis)
```

We want our outcome variable to be binary. Let's create a new outcome variable, `bin_out`, which will be 1 if `diagnosis == 'M'` and 0 if `diagnosis == 'B'.

```{r}
bc <- bc %>% mutate(bin_out = ifelse(diagnosis == "M", 1, 0)) %>% relocate(bin_out)
```

## Full Model

First, we want to establish logistic model using all variables in the dataset. We will do this by performing a Newton Raphson optimization in order to find the MLEs of the beta coefficients.

The likelihood function for a logistic model is defined as follows:

$$
f(\beta_0, \beta_1,...,\beta_{30}) = \sum_{i=1}^n \left(Y_i\left(\beta_0 + \sum_{j=1}^{30} \beta_jx_{ij}\right) - \log(1+e^{\left(\beta_0 + \sum_{j=1}^{30} \beta_jx_{ij}\right)}\right)
$$

Let $\pi_i = \frac{e^{\beta_0 + \sum_{j=1}^{30} \beta_jx_{ij}}}{1+e^{\beta_0 + \sum_{j=1}^{30} \beta_jx_{ij}}}$. Then, the gradient of this function is defined as follows:

$$
\triangledown f(\beta_0, \beta_1, ..., \beta_{30}) = 
\left(
\begin{matrix}
\sum_{i=1}^n Y_i-\pi_i \\ 
\sum_{i=1}^n x_{i1}(Y_i - \pi_i)\\
\sum_{i=1}^n x_{i2}(Y_i - \pi_i)\\
\vdots \\
\sum_{i=1}^n x_{i30}(Y_i - \pi_i)
\end{matrix}
\right)
$$
Finally, we define the Hessian of this function as follows:

$$
\begin{aligned}
\triangledown^2f(\beta_0, \beta_1, ..., \beta_{30}) &=  -\sum_{i=1}^n \left(
\begin{matrix}
1 \\ x_{i1} \\ x_{i2} \\ \vdots \\ x_{i30}
\end{matrix}
\right) \left(1 \,\, x_{i1} \,\, x_{i2} \,\, \dots \,\, x_{i30}\right)\pi_i(1-\pi_i)
\\
&= -\left(
\begin{matrix}
\sum_{i=1}^n \pi_i(1-\pi_i) &\quad \sum_{i=1}^n x_{i1}\pi_i(1-\pi_i) &\quad \dots &\quad \sum_{i=1}^nx_{i30}\pi_i(1-\pi_i) \\
\sum_{i=1}^n x_{i1}\pi_i(1-\pi_i) &\quad\sum_{i=1}^n x_{i1}^2\pi_i(1-\pi_i) &\quad \dots &\quad \sum_{i=1}^n x_{i30}x_{i1}\pi_i(1-\pi_i) \\
\sum_{i=1}^n x_{i2}\pi_i(1-\pi_i) &\quad\sum_{i=1}^n x_{i1}x_{i2}\pi_i(1-\pi_i) &\quad \dots &\quad \sum_{i=1}^n x_{i30}x_{i2}\pi_i(1-\pi_i) \\
\vdots &\quad \ddots &\quad \ddots &\quad \vdots \\
\sum_{i=1}^n x_{i30}\pi_i(1-\pi_i) &\quad\sum_{i=1}^n x_{i1}x_{i30}\pi_i(1-\pi_i) &\quad \dots &\quad \sum_{i=1}^n x_{i30}^2\pi_i(1-\pi_i) 
\end{matrix}
\right)
\\
&= \left(1 \,\, x_{i1} \,\, x_{i2} \,\, \dots \,\, x_{i30}\right) I(\pi_i(1-\pi_i))\left(
\begin{matrix}
1 \\ x_{i1} \\ x_{i2} \\ \vdots \\ x_{i30}
\end{matrix}
\right)
\end{aligned}
$$

Let's create a function that produces the log-likelihood, gradient vector, and hessian matrix, given a dataset and beta vector:

```{r}
rep_col <- function(x, n){
  matrix(rep(x, each = n), ncol = n, byrow = TRUE)
}

logistic_stuff <- function(dat, beta){
  
  x <- dat[[1]] %>% unname() %>% as.matrix()
  y <- dat[[2]] %>% unname() %>% as.matrix()
  
  x_with_1 <- cbind(1, x) 
    
  u <- x_with_1 %*% beta
 # return(u)
  
  expu <- exp(u)
  
  loglik <- sum(y*u - log(1 + expu))

  p <- expu/(1 + expu)
  #  return(p)
  # return(p)
  grad <- t(x_with_1) %*% (y - p)
  
  i_mat <- diag(nrow(p))
  diag(i_mat) <- p*(1 - p)

  hess <- -(t(x_with_1) %*% i_mat %*% x_with_1)

  return(
    list(
    loglik = loglik,
    grad = grad,
    hess = hess
  ))
}
```

### Newton-Raphson Algorithm

Now, let's write a Newton-Raphson algorithm to find the beta coefficients that maximize this function's likelihood. 

The unmodified estimate of $\theta_i = \left[\begin{array}{l} \beta_0 \\ \vdots \\ \beta_30 \end{array}\right]$ at each step $i$ of the Newton Raphson algorithm is: 
$$
\theta_i = \theta_{i-1} - [\triangledown^2f(\theta_{i-1})]^{-1}\triangledown f(\theta_{i-1})
$$

In this modified Newton Raphson algorithm, we want to first ensure that the $\triangledown^2f(\theta_{i-1})$ is either negative definite or replaced with a similar matrix that is negative definite. To do this, we will update the algorithm to be as follows:

$$
\theta_i = \theta_{i-1} - [\triangledown^2f(\theta_{i-1}) - k I]^{-1}\triangledown f(\theta_{i-1}),
$$

where $I$ is the identity matrix and $k$ is a constant that allows $\triangledown^2f(\theta_{i-1}) - kI$ to be negative definite. If $\triangledown^2f(\theta_{i-1})$ is already negative definite, $k$ will be 0.

Next, we want to add step-halving into our algorithm. We will proceed as follows:

$$
\theta_i = \theta_{i-1} - \frac{1}{2^j}[\triangledown^2f(\theta_{i-1}) - k I]^{-1}\triangledown f(\theta_{i-1}),
$$

where $j$ is chosen in a stepwise fashion, until $f(\theta_i) > f(\theta_{i-1})$.

```{r}

NewtonRaphson <- function(dat, func, start, tol = 1e-8, maxiter = 200) {
  i <- 0
  cur <- start
  stuff <- func(dat, cur)
  res <- c(0, stuff$loglik, cur)
  prevloglik <- -Inf
  
  while (i < maxiter && abs(stuff$loglik - prevloglik) > tol && !is.na(stuff$loglik)) {
    i <- i + 1
    prevloglik <- stuff$loglik
    prev <- cur
    newhess <- ((stuff$hess + t(stuff$hess))/2)
    
    if (!is.negative.definite(newhess)) { # redirection
     while (!is.negative.definite(newhess)) {
       # subtracts identity matrix until a negative definite matrix is achieved
        newhess1 <- newhess - diag(nrow(newhess)) 
       # sanity check print("changing ascent direction")
        newhess <- ((newhess1 + t(newhess1))/2)
      }
    }
    
    cur <- prev - solve(newhess) %*% stuff$grad
    stuff <- func(dat, cur)
    
    if (stuff$loglik < prevloglik) {  # back tracking (half-step)
      j = 1
      while (stuff$loglik < prevloglik & (!is.na(stuff$loglik))) {
         halfstep = 1/(2^j)
         cur <- prev - halfstep*solve(newhess) %*% stuff$grad
         stuff <- func(dat, cur)
        # sanity check print("backtracking")
         j = j + 1
      }
    }
    res <- rbind(res, c(i, stuff$loglik, cur))
  }
  return(res)
}
```

Let's start with all beta coefficients being 0.001.

```{r, warning=FALSE, message=FALSE}

beta_init <- rep(0.0000001, 31) %>% as.matrix()

test1 <- logistic_stuff(
  list(x = bc[,-c(1,2, 3)] %>% as.matrix(), 
       y = bc$bin_out %>% as.matrix()), 
  beta = beta_init)

ans <- NewtonRaphson(
      list(x = bc[,-c(1,2, 3)] %>% as.matrix(), 
       y = bc$bin_out %>% as.matrix()),
       logistic_stuff, 
       beta_init)
ans
```

The beta estimates are as follows:

```{r, warning=FALSE, message=FALSE}

if (sum(is.na(ans[nrow(ans),])) > 0) {
  beta_est <- ans[nrow(ans) - 1, -c(1,2)]
}

if (sum(is.na(ans[nrow(ans),])) == 0) {
  beta_est <- ans[nrow(ans), -c(1,2)]
}

tibble(beta_subscript = seq(0, 30), beta_estimates = beta_est) %>% knitr::kable()
```

### GLM

```{r}
glm(bin_out ~ ., data = bc[,-c(2, 3)], family="binomial")


```


## Logistic-Lasso Model

Now, we want to establish a logistic-lasso model, in which we want to minimize the weighted residual sum of squares of the logistic regression.

$$
\begin{aligned}
f(\beta_0, \beta_1, \dots, \beta_{30}) &\approx -\frac{1}{2n}\sum_{i=1}^n w_i\left(z_i -
\bf{(1 \quad x_i)}
\left(\begin{matrix}
\beta_0
\\
\beta_1
\\
\vdots
\\
\beta_{30}
\end{matrix}\right)
\right)^2 + C(\tilde{\beta_0}, \tilde{\beta_1}, \dots, \tilde{\beta_{30}})
\\
z_i &= (\bf{1 \quad x_i})
\left(\begin{matrix}
\tilde{\beta_0}
\\
\tilde{\beta_1}
\\
\vdots
\\
\tilde{\beta_{30}}
\end{matrix}\right) 
+ \frac{y_i - \tilde{p_i}(x_i)}{\tilde{p_i}(x_i)(1-\tilde{p_i}(x_i))}
\\
w_i &= \tilde{p_i}(x_i)(1-\tilde{p_i}(x_i))
\\
\tilde{p_i}(x_i) &= \frac{\exp\left((\bf{1 \quad x_i})
\left(\begin{matrix}
\tilde{\beta_0}
\\
\tilde{\beta_1}
\\
\vdots
\\
\tilde{\beta_{30}}
\end{matrix}\right)\right)}{1 + \exp\left((\bf{1 \quad x_i})
\left(\begin{matrix}
\tilde{\beta_0}
\\
\tilde{\beta_1}
\\
\vdots
\\
\tilde{\beta_{30}}
\end{matrix}\right)\right)}
\end{aligned}
$$

### Quadratic Approximation to the Log-likelihood

```{r}

quad_loglik <- function(dat, beta){ # beta vector includes beta_0
  
  x <- dat[[1]] %>% unname() %>% as.matrix()
  y <- dat[[2]] %>% unname() %>% as.matrix()
  
  
  x_with_1 <- cbind(1, x) 
    
  u <- x_with_1 %*% beta 
  expu <- exp(u)
  
  p <- expu/(1 + expu) # estimated outcome probability
  w_i <- p * (1 - p)  # weights
    
  z_i <- x_with_1 %*% beta + (y - p)/(p * (1 - p)) # working response

  loglik <- -(1/(2*nrow(x))) * t(w_i) %*% ((z_i - x_with_1 %*% beta)^2)
  
  return(loglik)
}
```

```{r}
beta_init <- rep(0.01, 31) %>% as.matrix()

test_quad <- quad_loglik(
  list(x = bc[,-c(1,2, 3)] %>% as.matrix(), 
       y = bc$bin_out %>% as.matrix()), 
  beta = beta_init)

test_quad
```


### Lasso Minimization

We want to achieve the following minimization:

$$
\begin{aligned}
\min_{(\beta_0, \beta_1, \dots, \beta_{30})}L(\beta_0, \beta_1, \dots, \beta_{30}, \lambda) &= \left\{-l(\beta_0, \beta_1, \dots, \beta_{30}) + \lambda \sum_{j=0}^{30}|\beta_j|\right\}
\end{aligned}
$$





