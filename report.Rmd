---
title: |
  | P8160 - Less is More: 
  | Comparing Logistic and Lasso-Logistic Regression in Breast Cancer Diagnosis
author: |
  | Group 1
  |
  | Amy Pitts, Hun Lee, Jimmy Kelliher,
  | Tucker Morgan, Waveley Qiu
date: "2022-04-01"
output:
  pdf_document: default
  
header-includes:
  - \usepackage{amsmath,amssymb,amsthm,amsfonts,bbm,graphics,enumitem}
  - \usepackage{geometry,mathrsfs}
  - \usepackage{tikz}
  - \usepackage{pgfplots}
  - \usetikzlibrary{shapes,arrows}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

source("shared_code/setup.R")
load("plotting_data.RData")

library(tidyverse)
library(lares) # for the correlation plot 
library(gtsummary)

## loading in the full dataset. 
bc <- read.csv("data/breast-cancer.csv") %>% 
  as_tibble() %>% 
  select(-id) %>% 
  janitor::clean_names()

```


\vspace{75mm}

\begin{center}
{\bf Abstract}
\end{center}

|        With the advancement of research methods and measurement techniques, there might exist a desire among researchers to make use of as much of the data as is collected using these new tools in the construction of predictive models. We sought to evaluate this inclination by constructing two models from a breast cancer imaging dataset to predict diagnosis outcome: a logistic-linear model, which contains all predictors of interest and has coefficients selected by a Newton-Raphson algorithm to maximize likelihood, and a logistic-lasso model, which contains an optimal number of predictors as controlled by a penalty term and has coefficients selected by a coordinate descent algorithm. When testing the performance of these two models in predicting diagnosis outcome, we found that the smaller, logistic-lasso model out-performed the larger, logistic-linear model in both of our measures of model fit (AUC and specificity). Though results from similar investigations may vary from dataset to dataset, our case study presents evidence for the advantage of considering parsimony in constructing predictive models.

\newpage
# 1. Introduction

## 1.1. Background

As breast cancer has become one of the most common kinds of cancer in the United States, substantial efforts have been made to aid in early and accurate detection. Along with national public health initiatives encouraging women at certain ages to schedule mammograms, great strides have been made to improve tumor imaging technology used in screening procedures. Such improvements have allowed clinicians more ways than ever to diagnose a patient, as an abundance of measurements are available and can be used to construct methods to evaluate disease severity. However, data does not always equate to information [1]. With more data comes more noise, and it becomes more important for statisticians and medical practitioners to separate signal from that noise.

## 1.2. Objectives

In this paper we investigate two questions: does having more data always correspond to an advantage in diagnosis prediction? Can we reduce the amount of data we need to collect while maintaining (or increasing) predictive power?

To this end, we use a breast cancer imaging dataset (detailed below) to fit two different models with the goal of predicting patient diagnosis outcomes. The first is a generalized linear-logistic regression model with a full set of predictors (i.e., "full model"), calculated using a Newton-Raphson optimization algorithm. The second is a penalized logistic-LASSO model (i.e., "optimal model"), which is capable of reducing the number of selected predictors from our dataset. This is implemented using a path-wise coordinate-descent optimization algorithm, and we utilize five-fold cross validation to obtain the optimal $\lambda$ penalization term. The algorithms for each method are discussed below in the methods section and corresponding code can be found in the appendix.

# 2. Methods

## 2.1. Data Cleaning and Exploratory Analysis

The data set of interest contains 569 rows and 32 columns related to breast tissue imaging with each entry representing an individual patient. The outcome of interest is patient diagnosis, taking on values of either malignant or benign. One column contains information about patient ID, which will be removed from our dataset. The other 30 columns correspond to summary measures (mean, standard deviation, and maximum) of variables such as radius, texture, perimeter, area, smoothness, compactness, concavity, concave points, symmetry, and fractal dimension. This dataset does not contain any missing values.

```{r predictor correlation plot, message=FALSE, warning=FALSE, include=FALSE}
bc_corr <- bc %>% 
  mutate(y = ifelse(diagnosis == "M", 1, 0)) %>% 
  relocate(y)  %>% 
  select(-c(y, diagnosis))

cor_cov_all <- cor(bc_corr)

bad <- c(
  "area_mean", "area_worst", "perimeter_mean", "perimeter_worst", "radius_mean"
  , "perimeter_se", "area_se"
  , "concave_points_worst", "concavity_mean"
  , "texture_worst"
)
```

In a quick exploration of the data, we find many of the predictors are highly correlated with one another. In **Figure 1**, we see a heat map correlation plot where several variables have dark blue coloring representing strong relationships. High collinearity between predictors can cause major issues in regression methods, particularly with high-dimensional data. To explore the correlation further, **Figure 2** shows the 25 largest correlations in our data. We see that the highest correlation is between radius mean and perimeter mean with a correlation value of 0.998 (maximum of 1). In the graph there are 21 combinations of variables that achieve a correlation greater than 0.90, which is a cause for concern in this context. We can break these 21 pairings into equivalence classes for further inspection.

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width = 8, fig.asp=0.8}
title = "Figure 1: Correlation Heat Plot of all Covariates"
corrplot::corrplot(cor_cov_all, 
                   method = "color",
                   type = "lower", 
                   insig = "blank",
                   diag = FALSE,
                   title=title,
                   mar=c(0,0,1,0)) 
```

The first grouping that are all mutually correlated is {`area_mean`, `area_worst`, `perimeter_mean`, `perimeter_worst`, `radius_mean`, `radius_worst`}. This grouping represents 15 of the correlation pairs in **Figure 2**.  Mathematically, if we consider the equivalence classes of variables that are highly correlated, these six variables would belong to the same equivalence class. To identify the best proxy for this grouping we look at the highest mean correlation which turns out to be `radius_worst`. The next grouping of correlated variables is {`radius_se`, `perimeter_se`, `area_se`}. The best representative will be `radius_se`. Next we can group {`concavity_mean`, `concave_points_worst`, `concave_points_mean`} together, and we find the best proxy variable is `concave_points_mean`. Finally, {`texture_mean`, `texture_worst`} is our last grouping with `texture_mean` being the variable saved. Thus from all the grouping and saving only the best proxy we will be removing `r length(bad)` variables leaving `r 30 - length(bad)` in our dataset. All the predictors used can be seen in Table 1.

```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
# this doesn't work in pdf files so it is not being run 
corr_cross(bc_corr, # name of dataset
  top = 25 # display top 10 couples of variables (by correlation coefficient)
)  +
  geom_text(aes(label =  round(corr,3)), hjust = 1.3) +
  labs(title= "Figure 2: Ranked Cross-Correlations")
```

![](images/cor_rank.png){width=80%}

In Table 1 we see that there are 357 benign (B) cases and 212 malignant (M) cases. To implement both the full and optimal model the data set will be split into train and test sets using an 80-20 split. The data is standardized before fitting both of our models to help with comparability. For LASSO regression in particular, it is best practice to center and scale data. The $\ell_1$-norm penalization in LASSO regression will unequally penalize coefficient estimates if the covariates are of different magnitudes or scales. Therefore, standardizing our data ensures equal weighting and penalization.

**Table 1: Patient Characteristics**
```{r echo=FALSE, message=FALSE, warning=FALSE}
remove_bad_vars <- function(indat, bad_vars){
  outdat <-
    indat %>% 
    dplyr::select(-bad_vars)
    return(outdat)
}
bc <- remove_bad_vars(bc, bad)

tbl_summary(bc, by = diagnosis,
            statistic = list(all_continuous() ~ "{mean} ({sd})",
                     all_categorical() ~ "{n} / {N} ({p}%)"),
    digits = all_continuous() ~ 2) %>% 
  add_p()  %>% 
  modify_header(label ~ "**Variable**") %>%
  modify_spanning_header(c("stat_1", "stat_2") ~ "**Diagnosis Received**")# %>%
  #bold_labels() 
  #modify_caption("**Patient Characteristics** (N = {N})")

```



## 2.2 Newton-Raphson Algorithm 

The Newton-Raphson method is an efficient algorithm for computing the maximum likelihood estimator (MLE) under certain conditions. However, to even consider computing an MLE, we need to choose a reasonable model for our data so that we can construct our likelihood function.

\vspace{5mm}

Toward that aim, let $\mathbf{X} \equiv (x_{ij})$ denote the $n \times p$ matrix corresponding to our $n = 569$ observations and our $p = 21$ features (including an intercept term). Because our outcome $\mathbf{Y} \equiv (y_1, \ldots, y_n)^t$ is binary, it is natural to model $y_i \, | \, x_{i1}, \ldots, x_{ip} \sim \text{Bernoulli}(\pi_i)$, where
	\begin{align*}
		\pi_i
		&\equiv P(y_i = 1 \, | \, x_{i1}, \ldots, x_{ip}) \\
		&= \left( 1 + \exp \left( - \sum_{j = 1}^p \beta_j x_{ij} \right) \right)^{-1}, \quad \text{such that} \quad
	\end{align*}
	\[ \log \left( \frac{\pi_i}{1 - \pi_i} \right) = \sum_{j = 1}^p \beta_j x_{ij} \]
for a vector of parameters $\boldsymbol\beta \in \mathbb{R}^p$. The likelihood function of $\boldsymbol\beta$ given our data is
	\begin{align*}
		\mathcal{L}(\boldsymbol\beta \, | \, \mathbf{X}, \mathbf{Y})
		&\equiv f(\mathbf{X}, \mathbf{Y} \, | \, \boldsymbol\beta) \\[10pt]
		&= \prod_{i = 1}^n \pi_i^{y_i} (1 - \pi_i)^{1 - y_i} \\
		&= \prod_{i = 1}^n \left( \frac{\pi_i}{1 - \pi_i} \right)^{y_i} (1 - \pi_i).
	\end{align*}
To employ the Newton-Raphson procedure, we need to compute the gradient vector and hessian matrix corresponding to $\mathcal{L}$. However, as computing derivatives of the likelihood function is tedious, we instead consider the log-likelihood function
	\begin{align*}
		l(\boldsymbol\beta \, | \, \mathbf{X}, \mathbf{Y})
		&\equiv \log \mathcal{L}(\boldsymbol\beta \, | \, \mathbf{X}, \mathbf{Y}) \\
		&= \log \left( \prod_{i = 1}^n \left( \frac{\pi_i}{1 - \pi_i} \right)^{y_i} (1 - \pi_i) \right) \\
		&= \sum_{i = 1}^n \left( y_i \log \left( \frac{\pi_i}{1 - \pi_i} \right) - \log \left( \frac{1}{1 - \pi_i} \right) \right) \\
		&= \sum_{i = 1}^n \left( y_i \sum_{j = 1}^p \beta_j x_{ij} - \log \left( 1 + \exp \left( \sum_{j = 1}^p \beta_j x_{ij} \right) \right) \right).
	\end{align*}
Toward computing the derivative of the $l$, we define
	\[ \eta_i \equiv \sum_{j = 1}^p \beta_j x_{ij} \]
for each $i \in \{ 1, \ldots, n \}$. Observe that for each $k \in \{ 1, \ldots, p \}$,
	\[ \frac{\partial \eta_i}{\partial \beta_k} = x_{ik} \quad \text{and} \quad \frac{\partial \pi_i}{\partial \beta_k} = x_{ik} \pi_i (1 - \pi_i). \]
Thus, it follows that
	\begin{align*}
		\frac{\partial l}{\partial \beta_k}
		&= \frac{\partial}{\partial \beta_k} \left( \sum_{i = 1}^n \Big( y_i \eta_i - \log \left( 1 + e^{\eta_i} \right) \Big) \right) \\
		&= \sum_{i = 1}^n \left( y_i \frac{\partial \eta_i}{\partial \beta_k} - \frac{e^{\eta_i}}{1 + e^{\eta_i}} \frac{\partial \eta_i}{\partial \beta_k} \right) \\
		&= \sum_{i = 1}^n x_{ik}(y_i - \pi_i), \quad \text{and}
	\end{align*}
	\begin{align*}
		\frac{\partial l^2}{\partial \beta_k \beta_l}
		&= \frac{\partial}{\partial \beta_l} \left( \sum_{i = 1}^n x_{ik}(y_i - \pi_i) \right) \\
		&= - \sum_{i = 1}^n x_{ik} \frac{\partial \pi_i}{\partial \beta_l} \\
		&= - \sum_{i = 1}^n x_{ik} x_{il} \pi_i (1 - \pi_i).
	\end{align*}
for $k, l \in \{ 1, \ldots p \}$. The above expressions completely characterize the gradient vector and hessian matrix corresponding to the log-likelihood $l$. However, it will be convenient to express these objects compactly via matrix notation. Toward this aim, we define $\boldsymbol\pi \equiv (\pi_1, \ldots, \pi_n)^t$ to be a vector of probabilities and
	\[ \mathbf{W} \equiv \begin{pmatrix}
		w_1 & 0 & \cdots & 0 \\
		0 & w_2 & \cdots & 0 \\
		\vdots & \vdots & \ddots & \vdots \\
		0 & 0 & \cdots & w_n
	\end{pmatrix}, \quad \text{where} \quad w_i \equiv \pi_i (1 - \pi_i) \]
for each $i \in \{ 1, \ldots, n \}$. We can think of $\mathbf{W}$ as a pseudo-weight matrix, noting that its entries do not generally sum to unity. Given this notation, the gradient vector and hessian matrix corresponding to $l$ are given by
	\[
		\nabla l(\boldsymbol\beta \, | \, \mathbf{X}, \mathbf{Y}) = \mathbf{X}^t (\mathbf{Y} - \boldsymbol\pi)
			\quad \text{and} \quad
		\nabla^2 l(\boldsymbol\beta \, | \, \mathbf{X}, \mathbf{Y}) = - \mathbf{X}^t \mathbf{W} \mathbf{X},
	\]
respectively. Our Newton-Raphson algorithm is then characterized by the procedure
	\[ \boldsymbol\beta_{t + 1} = \boldsymbol\beta_t + (\mathbf{X}^t \mathbf{W} \mathbf{X})^{-1}  \mathbf{X}^t (\mathbf{Y} - \boldsymbol\pi). \]
To assess if this updating procedure is well-behaved, we must investigate the properties of the hessian matrix, thereby motivating the following proposition.

\vspace{5mm}

__Proposition 1.__ The hessian matrix of $l$ is negative semi-definite. If $\pi_i \in (0, 1)$ for at least one $i \in \{1, \ldots, n\}$, and if $\mathbf{X}$ is of full rank, then the hessian matrix is negative definite.

_Proof._ Because $\mathbf{W}$ is a diagonal matrix with non-negative elements, it is positive semi-definite. Thus, for any $u \in \mathbb{R}^p \setminus \{ \mathbf{0} \}$, it follows that $\mathbf{X} u \in \mathbb{R}^p$, and hence
	\begin{align*}
		u^t \nabla^2 l(\boldsymbol\beta \, | \, \mathbf{X}, \mathbf{Y}) u
		&= - u^t \mathbf{X}^t \mathbf{W} \mathbf{X} u \\
		&= - (\mathbf{X} u)^t \mathbf{W} (\mathbf{X}u) \tag{by rules for transpose of a product} \\
		&\leq 0 \tag{by positive semi-definiteness of $\mathbf{W}$}.
	\end{align*}
That is, the hessian matrix is negative semi-definite. If we further have that $\mathbf{X}$ is of full rank, then $\mathbf{X} u \in \mathbb{R}^p \setminus \{ \mathbf{0} \}$. Moreover, if $\pi_i \in (0, 1)$ for at least one $i \in \{1, \ldots, n\}$, then $\mathbf{W}$ is positive definite. Together, these facts make the above inequality strict, such that the hessian matrix is negative definite. $\Box$

\vspace{5mm}

The above result provides us with two key insights: (1) if our data matrix is not of full rank, the hessian matrix might not be negative definite; and (2) if our fitted probabilities are all either zero or one, then our hessian matrix will not be negative definite. By culling highly correlated covariates during the exploratory data analysis phase, we have precluded scenario (1) (and indeed, we find that the Newton-Raphson algorithm does not converge if all 30 covariates are considered). To prevent scenario (2), we seed our initial guess at $\boldsymbol\beta = \mathbf{0}$, which corresponds to $\pi_i = \frac{1}{2}$ for all $i \in \{1, \ldots, n\}$. As added precautions, we further control for ascent direction and allow step-halving, though this is not critical for such a well-behaved optimization problem.



## 2.3 Logistic LASSO Algorithm 

__Lemma 1.__ Consider the optimization problem
  \[ \min_{x \in \mathbb{R}}  \left\{ \frac{1}{2}(x - b)^2 + c|x| \right\} \]
for $b \in\mathbb{R}$ and $c \in \mathbb{R}_{++}$. It follows that the minimizer is given by
  \[ \hat{x} = S(b, c), \]
where $S$ is the soft-thresholding operator.

__Lemma 2.__ Consider the optimization problem
  \[ \min_{\beta_k \in \mathbb{R}} \left\{ \frac{1}{2n} \sum_{i = 1}^n w_i \left(z_i - \sum_{j = 1}^p \beta_j x_{ij} \right)^2 \right\} \]
for some $k \in \{ 1, \ldots, p \}$. It follows that the minimizer is given by
  \[ \hat{\beta}_k = \left( \sum_{i = 1}^n w_i x_{ik}^2 \right)^{-1} \sum_{i = 1}^n w_i x_{ik} \left(z_i - \sum_{j \neq k} \beta_j x_{ij} \right). \]

__Lemma 3.__ With $\hat{\beta}_k$ defined as above,
\begin{align*}
\min_{\beta_k \in \mathbb{R}}& \left\{ \frac{1}{2n} \sum_{i = 1}^n w_i \left(z_i - \sum_{j = 1}^p \beta_j x_{ij} \right)^2 + \lambda \sum_{j = 1}^p |\beta_j| \right\} \\
  &= \min_{\beta_k \in \mathbb{R}} \left\{ \frac{1}{2}(\beta_k - \hat{\beta}_k)^2 + \left( \frac{1}{n} \sum_{i = 1}^n w_i x_{ik}^2 \right)^{-1} \lambda |\beta_k| \right\}.
\end{align*}

__Proposition.__ By Lemma 1 and Lemma 3,
\begin{align*}
 \underset{\beta_k \in \mathbb{R}}{\arg \min} & \left\{ \frac{1}{2n} \sum_{i = 1}^n w_i \left(z_i - \sum_{j = 1}^p \beta_j x_{ij} \right)^2 + \lambda \sum_{j = 1}^p |\beta_j| \right\} \\
  &= S \left(\hat{\beta}_k, \left( \frac{1}{n} \sum_{i = 1}^n w_i x_{ik}^2 \right)^{-1} \lambda \right) 
 \end{align*} 
 
 **More info from the pres: NEed to be editted to paragraph form**
 
 
For vector $\boldsymbol\alpha \in \mathbb{R}^{p + 1}$, define $g : \mathbb{R}^{p+1} \to \mathbb{R}$ to be
  \[ g(\boldsymbol\beta) \equiv - \frac{1}{2n} \sum_{i = 1}^n w_i (z_i - \mathbf{X}_i^t \boldsymbol\beta )^2 + O(\boldsymbol\alpha), \]
the Taylor expansion of our log-likelihood centered around $\boldsymbol\alpha$, where
  \begin{align*}
    z_i &\equiv \mathbf{X}_i^t \boldsymbol\alpha + \frac{y_i - \pi_i}{w_i}, \tag{effective response} \\
    w_i &\equiv \pi_i (1 - \pi_i), \text{ and} \tag{effective weights} \\
    \pi_i &\equiv \frac{e^{\mathbf{X}_i^t \boldsymbol\alpha}}{1 + e^{\mathbf{X}_i^t \boldsymbol\alpha}}
  \end{align*}
for $i \in \{1, \ldots, n \}.$


It follows that for any $\lambda \in \mathbb{R}_+$,
  \[
    \underset{\beta_k \in \mathbb{R}}{\arg \min} \left\{ g(\boldsymbol\beta) + \lambda \sum_{j = 1}^p |\beta_j| \right\}
    = S \left(\hat{\beta}_k, \lambda_k \right), \text{ where}
  \]
  \begin{align*}
    \hat{\beta}_k &\equiv \left( \sum_{i = 1}^n w_i x_{ik}^2 \right)^{-1} \sum_{i = 1}^n w_i x_{ik} \left(z_i - \sum_{j \neq k} \beta_j x_{ij} \right), \\
    \lambda_k &\equiv \left( \frac{1}{n} \sum_{i = 1}^n w_i x_{ik}^2 \right)^{-1} \lambda,
  \end{align*}
and $S$ is the soft-thresholding (or _shrinkage_) function. This is analogous to a penalized, weighted Gaussian regression.


Our coordinate descent algorithm proceeds as follows.

- Outer Loop: Decrement over $\lambda \in (\lambda_{\max}, \ldots, \lambda_{\min})$ where $\lambda_{\max} = \frac{max|X^Ty|}{n}$
- Middle Loop: Update $\boldsymbol\alpha = \boldsymbol\beta$ and Taylor expand $g$ around $\boldsymbol\alpha$.
- Inner Loop: Update $\beta_k = S \left(\hat{\beta}_k, \lambda_k \right)$ sequentially for $k \in \{ 0, 1, \ldots, p, 0, 1, \ldots, p, 0, 1, \ldots \}$ until convergence.

To be specific, an outer loop is created for every new value of $\lambda$ and then we use coordinate descent to solve the penalized weighted least-squares problem. Namely, $z_i$, $w_i$, and $\pi_i$ are updated at the current parameters for every outer loop to update the quadratic approximation $g(\beta)$ and then keep updating parameters until it satisfies the threshold set beforehand.

Note: the middle loop terminates when a given Taylor expansion no longer yields updates (within the specified tolerance) to $\boldsymbol\beta$ in the inner loop.



## 2.4 Five-fold Cross Validation 

As the performance of the logistic LASSO model depends on the penalty factor ($\lambda$) that is selected, we will perform cross validation in order to determine the $\lambda$ that produces a model that optimizes a metric of interest. 

First, we need to define our range of possible $\lambda$ values, where the largest value produces a model with no predictors selected and the smallest value would produce a model with all predictors selected. While we could make $\lambda_{max}$ infinitely large, we will define the maximum value as as the smallest penalty for which $\beta_k=0$ for all $k\in\{1, \dots, p\}$. This value works out to be the maximum of the inner product between our predictors ($X$) and our outcome ($Y$) from the imaging dataset. Likewise, while we could make $\lambda_{min}$ infinitely small (but greater than 0), we will select the largest value that produces the full model, which is identical to the model produced by the Newton Raphson algorithm we have previously defined. As recommended by Friedman et. al [2], we started by setting the smallest value in our range to be $\lambda_{min} = \frac{\lambda_{max}}{1000}$. However, we were not able to use this value as as the minimum $\lambda$ in our range because it did not select the full model. Thus, we used the suggested $\frac{\lambda_{max}}{1000}$ as a starting point and empirically derived our $\lambda_{min}$ by halving the working value a few times -- ultimately, we saw that a value of $0.0001\,\,\left(\approx\frac{\lambda_{max}}{4000}\right)$ would be a suffice as a sufficiently small penalty factor that would select the full model. We then constructed a sequence between $\log{\lambda_{max}}$ and $\log{\lambda_{min}}$ so that the sequence would contain 100 values with equal intervals between values and exponentiated all values in that sequence. In this fashion, we thus created a range of 100 values between $\lambda_{max}$ and $\lambda_{min}$ a log scale. 

After defining this range of $\lambda$ values, we implemented a five-fold cross validation algorithm to identify the best $\lambda$. In **Figure 3** the first step of the process is to split the full dataset into train and test data as described above. The test data will not be touched until an optimal $\lambda$ is selected and final method comparison is performed. Using just the training data the cross validation procedure implements a five-fold process where the test data is split into five "folds" or chunks. During the first iteration, the first fold of data is used as a test or "validation" set while the four other splits are used as the training set. The second iteration will use the second fold as the validation set and the remaining folds as the training set. This process continues for each fold, producing an Area-Under-the- ROC-Curve (AUC) value for each $\lambda$ in our range of values. For example, in one fold we have 100 $\lambda$ values all with one corresponding AUC value. Thus, for five folds each $\lambda$ will have five associated AUC values. 

To select our optimal tuning parameter, we use two measures: greatest mean AUC and Minimax AUC from 5-fold cross validation. To be specific, we are using 80% of training set in each split five times to compute mean AUC for each 100 $\lambda$ values and selects the lambda which gives the greatest mean AUC as well as to compute maximum $1-AUC$ and selects the lambda which gives the minimum of the maximum $1-AUC$. The greatest mean AUC is to account for the scenario where we achieve the highest predictive ability to distinguish between classes (separabiltiy) on average and minimax AUC is to account for the scenario where we aim to minimize the possible loss for the worst case scenario. The greatest mean AUC and Minimax AUC optimal models are compared against the full model below.

**Figure 3: Cross Validation Procedure**

![](images/cv_vis.png){width=105%}

As we implemented the five-fold cross validation process, we grew concerned over the potential of our step size between $\lambda$ values being too large and that the changes between models selected by consecutive were too dramatic. To increase the precision of our estimate and to prevent the process from removing more than one variable between increasing penalty values, we modified the simple cross validation procedure to include a grid search around perceived maximums when any two models corresponding with consecutive $\lambda$ values differed by the inclusion/exclusion of more than one predictor after a five-fold cross validation process had been completely run through. When this occurred, we created a new range of possible $\lambda$ values centered around the $\lambda$ value ($\hat{\lambda}_{max}$) that had been identified to maximize the mean of the five AUC values produced from the cross validation process. 

We will create our new range using the same logorithmic-exponentiation process we used to construct our initial range of $\lambda$ values. Using the same (unexponentiated) step size between $\log\lambda$ values in the original sequence of values, we defined the maximum of the new sequence of values to be two of those steps above $\log\hat{\lambda}_{max}$ and the minimum to be two steps below. Finally, we constructed a uniform sequence of the same length as the original sequence (consisting of 100 values) between the two endpoints and exponentiated the result to produce our new truncated range of $\lambda$ values. From here, we will run the five-fold cross validation again, and repeat the process until consecutive $\lambda$ values do not remove more than one predictor between models as $\lambda$ increases.


## 2.5 Final Model Evaulation

Each optimal $\lambda$ selected is then used to fit a final model on the full training dataset. To compare these optimal models with the full (Newton-Raphson) model, we use the $\beta_j$ values specified by each to make classification predictions based on the test dataset, which is held out of analysis prior to this step. In other words, the $\beta_j$ values in each model are used in conjunction with the test data, $X_{ij}$, to predict class probabilities for each testing data observation. These predicted probabilities are compared with the observed outcomes in the testing data to produce receiver operating characteristic (ROC) curves, as well as AUC, sensitivity, and specificity values.

# 3. Results 

## 3.1 Cross-Validation Results

Evaluating the cross validation results, we can first confirm that our range of $\lambda$ values has one maximum AUC value when using the greatest mean AUC optimal lambda method. This can be seen in **Figure 4** depicting the largest AUC and corresponding lambda values by the vertical dotted line. After the vertical dotted line the AUC values decrease thus indicating one maximum values exists in our range. 

**Figure 4: Cross Validation Results: Selecting Best Lambda**

```{r, warning=FALSE, message=FALSE, echo=FALSE}
auc_vs_lambda <- 
  cv_res[[1]][[1]] %>% 
  data.frame() %>% 
  ggplot(x = lambda, y = mean_auc) +
  geom_line(aes(x = lambda, y = mean_auc), col = "black") +
  geom_vline(xintercept = selected_lambda, linetype = "dashed", color = "red") +
  geom_point(aes(x = lambda, y = mean_auc), col = "black") +
  geom_text(aes(x = selected_lambda, y = 0.96, label = "Lambda - Greatest Mean AUC = 0.0101"), col = "red", size = 3, hjust = -0.05) +
#  coord_cartesian(xlim = c(0, 0.2), ylim = c(.9, 1)) +
  labs(title = "Mean AUC vs. Lambda",
       x = "Lambda",
       y = "Mean AUC")
auc_vs_lambda
```

We can also evaluate our cross validation results by looking at the beta estimates for each corresponding $\lambda$ value. This is seen in **Figure 5**. The smallest value of -log(lambda) corresponds with our null model where all estimates of $\beta_j$ are zero and the $\lambda$ penalization term is large. The largest value of -log(lambda) corresponds with a full model where no $\beta_j$ values are zero. The vertical lines depict the optimal lambda value chosen for each of our selection methods. We see that the greatest mean AUC selects a larger $\lambda$ value, greater penalization, and thus a model with fewer predictor coefficients compared to the minimax AUC method. 

**Figure 5: Cross Validation Results: LASSO Coefficients**

```{r, warning=FALSE, message=FALSE, echo=FALSE}
beta_coef_plot <- 
  lfr_df %>% 
  group_by(lambda) %>% 
  filter(beta_coef != "intercept") %>% 
  ggplot(x = -log(lambda), y = coef_est, group = beta_coef) +
  geom_path(aes(x = -log(lambda), y = coef_est, group = beta_coef, col = beta_coef)) +
  geom_vline(xintercept = -log(selected_lambda), col = "black", linetype = "dashed") + 
  geom_text(aes(x = -log(selected_lambda), y = 5, label = "Lambda - Greatest Mean AUC"), colour = "black", size = 3, hjust = 1.05) +
  geom_vline(xintercept = -log(selected_lambda_minmax), col = "black", linetype = "dashed") +
  geom_text(aes(x = -log(selected_lambda_minmax), y = 5, label = "Lambda - Minimax AUC"), colour = "black", size = 3, hjust = 1.05) +
  labs(title = "Beta Coefficients in LASSO Model", y = "Coefficient Estimate", x = "-log(lambda)") + theme(legend.position = "none")
beta_coef_plot
```


## 3.2 Model Validation Results 

To compare the beta estimates in our full model and two optimal models, we can look at **Table 2**. Please note that we standardized our data so care needs to be taken before interpreting these values in the context of the project. Also note that care needs to be taken when interpreting the beta values for the optimal model because of standardization and the lambda penalty term. In **Table 2** we see that the greatest mean AUC has the most beta values equal to zero. **Table 3** also confirms that greatest mean AUC produces a model with 6 nonzero beta values with lambda value 0.0101, not including the intercept term. Minimax optimization selects 16 nonzero beta values with lambda value 0.0006. While each model has test AUC values above 0.97, the model with the largest test AUC is the greatest mean optimal model.

**Table 2: Beta Coefficients Comparing Full and Optimal Models**

![](images/beta_comp_table_cropped.png){width=85%} <!--{height=70%}-->

While the AUC value is a good indicator of overall classification performance, we are still confronted with the trade-off between sensitivity and specificity. Imagining is typically the first line of defense for diagnosing cancers, so optimizing sensitivity is usually of great interest. In **Table 3** we have displayed the largest specificity value achieved while realizing sensitivity to equal 1 (all positive individuals correctly identified as positive). Namely, we are looking for a model that sacrifices the least value of specificity while still achieving 1 sensitivity. We can see in **Table 3**, the greatest mean AUC lasso model has the highest specificity, 69.23%. Though minimax AUC Lasso model is the minimum of the worst AUC model, it still achieves much larger specificity than the full model, 55.38%. Namely, if we choose to a lasso model, it is still expected to manage to achieve 55.38% specificity while maintaining 1 sensitivity for the worst case scenario. It is to be observed that full model has the lowest sensitivity, 24.62%, and hence needs to sacrificice specificity more than 75% to achieve 1 sensitivity. This sensitivity and specificity trade-off can also be seen in our ROC curves displayed in **Figure 6**. 

**Table 3: Model Summary Comparing Full and Optimal Models**

![](images/model_summ_table_cropped.png){width=85%}


**Figure 6: Comparing Full and Optimal Models Performance**

![](images/roc_plot.png){width=75%}



# 4. Discussion 

## 4.1. Summary of Findings

Comparing the Newton-Raphson full model against two optimal models (greatest mean AUC, Minimax AUC) the model that out-preformed the others is the greatest mean AUC optimal model. This greatest mean AUC optimal model had the fewest predictors, highest AUC value, and highest specificity when maximizing sensitivity. Of course the ideal performance is to accurately classify every patient, AUC = 1; although very close, our test performance does not reach AUC = 1. Therefore, we need to balance sensitivity and specificity, to determine the lesser of two errors: false positives vs false negatives, when setting decision boundaries.

It is clear from our results that having more data does not always correspond to an advantage in diagnosis prediction. We found better prediction with many fewer predictors, six, in our optimal model compared to a full 20 in the model incorporating the most data. Given these results, it may benefit clinicians and practitioners to focus on certain indicators or attributes of breast imaging data in an effort to separate signal from noise.

Lastly, it is to be observed that 6 variables, *texture_mean*, *concave_points_mean*,*radius_worst*,*smoothness_worst*,*concavity_worst*, and *symmetry_worst*, play important roles in predicting whether breast imaging data is malignant.

## 4.2. Limitations

While the initial reduction of variables to limit high correlations was beneficial, we may have not selected the best representative of the correlation group. We did not try different representatives of the equivalence classes and so this might be a limitation for further study. Besides AUC as a metric of model performance, the accuracy rate of prediction can be also utilized as a means of measuring model performance if the information of the cutoff probability for classification is given from medical professionals.

We also see AUC values very close to a perfect 1.0 in each of the models considered. This could be due to very clean and unambiguous data, which limits our knowledge of how these models might actually perform in a "real-world" setting. In the future, it would be beneficial to examine different data sets in order to better understand how these models perform on breast imaging data.

## 4.3 Future Work

Two avenues of future work were discussed above, including the consideration of different model evaluation criteria based on clinician recommendation and implementation on more ambiguous data sets. Another interesting improvement could be the implementation of Monte Carlo Cross Validation. In this project, we performed five-fold cross validation once, however this procedure can be repeated multiple times with varied folds of the training data to obtain more stable estimates of the optimal $\lambda$ value. Additionally, models may perform better on larger datasets. Here, we only had 569 observations, but more observations could help us learn more about the relationships between the imaging data and diagnosis outcome.

## 4.4. Group Contributions

Our group worked in conjunction on many aspects of the project. Amy, Waveley, and Hun worked on developing and implementing the Newton-Raphson optimization algorithm. Jimmy worked on developing and implementing the LASSO coordinate-descent algorithm. Tucker and Waveley worked on the cross validation procedure and plotting results from these output. All group members worked on the project presentation and report in varying capacities.

\newpage

# References 

\begin{enumerate}[label={[\arabic*]}]
  \item  Duncan, J. R. (2017, September 1). Information overload: When less is more in medical imaging. De Gruyter. https://www.degruyter.com/document/doi/10.1515/dx-2017-0008/html?lang=en 
  \item Friedman J, Hastie T, Tibshirani R. Regularization Paths for Generalized Linear Models via Coordinate Descent. J Stat Softw. 2010;33(1):1-22. PMID: 20808728; PMCID: PMC2929880.
\end{enumerate}

\newpage 

# Appendices

## Appendix A: Comparison to GLMNET Estimations

![](images/glm_comp_table.png){width=100%}
\newpage

## Appendix B: Logistic Likelihood

```{r, eval=FALSE}
loglike_func <- function(dat, betavec){
  
  dat = bc_trn
  
  # x matrix
  dat_temp <-
    dat %>%
    mutate(intercept = 1) %>%
    select(-diagnosis) %>%
    relocate(intercept)
  
  dat_x <-  
    dat_temp %>%
    as.matrix() %>%
    unname() 
  
  # pi vector
  u <- dat_x %*% betavec
  pi <- exp(u) / (1 + exp(u))
  
  # loglikelihood
  loglik <- sum(dat[,1]*u - log(1 + exp(u)))
  
  #gradient 
  grad <- t(dat_x) %*% (dat[,1] - pi)
  
  # Hessian 
  W <- diag(nrow(pi))
  diag(W) <- pi*(1 - pi)
  hess <- -(t(dat_x) %*% W %*% (dat_x))
  
  return(list(loglik = loglik, grad = grad, hess = hess))
  
}
```

\newpage

## Appendix C: Newton Raphson Implementation

```{r}
NewtonRaphson <- function(dat,  start, tol = 1e-8, maxiter = 200){
  
  i <- 0
  cur <- start 
  stuff <- loglike_func(dat, cur)
  res <- c(i = 0, "loglik" = stuff$loglik,  "step" = 1, cur)
  
  prevloglik <- -Inf # to make sure it iterates
  
  while (i < maxiter && abs(stuff$loglik - prevloglik) > tol) {
    step <- 1
    i <- i + 1
    prevloglik <- stuff$loglik
    
    # check negative definite  
    eigen_vals <- eigen(stuff$hess)
    
    if (max(eigen_vals$values) <= 0 ) { # check neg def, if not change 
      hess <- stuff$hess
    } else { # if it is pos def then need to adjust 
      hess <- stuff$hess - (max(eigen_vals$values) + 0.1)*diag(nrow(stuff$hess))
    } 
    
    prev  <- cur
    cur   <- prev - step*(solve(stuff$hess) %*% stuff$grad)
    stuff <- loglike_func(dat, cur) # log-lik, gradient, Hessian
    
    # step halving
    while (stuff$loglik < prevloglik) {
      stuff <- loglike_func(dat, prev)
      step  <- step / 2 # this is where half steping happens 
      cur   <- prev - step*(solve(stuff$hess) %*% stuff$grad)
      stuff <- loglike_func(dat, cur)
    }
    # add current values to results matrix
    res <- rbind(res, c(i, stuff$loglik, step, cur))
  }
  
  colnames(res) <- c("i", "loglik",  "step", "intercept", names(dat[,-1]))
  return(res)
}
```

\newpage

## Appendix D: Logistic Lasso Implementation

```{r, eval=FALSE}
# logistic function
logistic <- function(x) 1 / (1 + exp(-x))

# shrinkage function
S <- function(beta, gamma) {
  if(abs(beta) <= gamma) {
    0
  } else if(beta > 0) {
    beta - gamma
  } else {
    beta + gamma
  }
}

# probability adjustment function
p_adj <- function(p, epsilon) {
  if (p < epsilon) {
    0
  } else if(p > 1 - epsilon) {
    1
  } else {
    p
  }
}

# weight adjustment function
w_adj <- function(p, epsilon) {
  if ((p < epsilon) | (p > 1 - epsilon)) {
    epsilon
  } else {
    p * (1 - p)
  }
}

# executes logistic lasso regression via coordinate descent
logistic_lasso <- function(
  # a numeric design matrix or data frame with named columns
  inputs
  # a vector of outputs; we must have length(output) == nrow(inputs)
  , output
  # a vector of descending penalization factors, ideally on a logarithmic scale
  , lambda_vec
  # standardize inputs using scale
  , standardize   = TRUE
  # a buffer to prevent divergence when fitted probabilities approach 0 or 1
  , epsilon       = 10^-8
  # maximum number of updates to quadratic approximation of likelihood
  , outer_maxiter =  100
  # maximum number of cycles for coordinate descent given quadratic approximation
  , inner_maxiter = 1000
  # tolerance for convergence of coordinate descent
  , tolerance     = 10^-12
) {

  # standardize data unless otherwise specified
  if(standardize) {

    # format data
    X <- as.matrix(cbind(rep(1, nrow(inputs)), scale(inputs)))
    y <- output  

  } else {

    # format data
    X <- as.matrix(cbind(rep(1, nrow(inputs)), inputs))
    y <- output    

  }

  # initialize coefficients at origin
  beta    <- rep(0, ncol(X))
  beta_df <- NULL

  # begin lambda decrement
  for(lambda in lambda_vec) {

    outer_term <- 0
    outer_iter <- 1

    # update quadratic approximation, execute coordinate descent until convergence, repeat
    while(outer_term < 1) {

      # update quadratic approximation; i.e., taylor expand around current estimates  
      p <- map_dbl(logistic(X %*% beta), p_adj, epsilon)
      w <- map_dbl(p, w_adj, epsilon)
      z <- X %*% beta + (y - p) / w

      inner_term <- 0
      inner_iter <- 1

      # given current quadratic approximation, execute coordinate descent
      while(inner_term < 1) {

        beta_old <- beta

        # execute a complete cycle of coordinate descent
        for(k in 1:ncol(X)) {

          # un-penalized coefficient update
          b_k_temp <- sum(w * (z - X[ , -k] %*% beta[-k]) * X[ , k]) / sum(w * X[ , k]^2)
          # shrinkage update
          b_k      <- S(b_k_temp, (k > 1) * lambda / mean(w * X[ , k]^2))
          # update coefficient vector
          beta[k] <- b_k

        }

        inner_iter <- inner_iter + 1

        if(inner_iter == inner_maxiter | max(abs(beta - beta_old)) < tolerance) {

          inner_term <- 1

        }

      }

      outer_iter <- outer_iter + 1

      if(outer_iter == outer_maxiter | inner_iter == 2) {

        outer_term <- 1

      }

    }

    beta_df <- rbind(beta_df, t(beta))

  }

  # format data frame of coefficient estimates
  colnames(beta_df) <- c("intercept", names(inputs))
  beta_df <- as_tibble(beta_df)

  # extract number of variables selected for each lambda
  selected_vec <- apply(beta_df, 1, function(x) sum(x != 0) - 1)

  # output results
  list(lambda = lambda_vec, beta = beta_df, selected = selected_vec)
}
```


\newpage

## Appendix E: Defining Lambda Range

```{r, eval=FALSE}
# lambda initialization function
lambda_init <- function(start, stop, step, func = identity){
  lambda_vec <- func(seq(start, stop, step))
  return(lambda_vec)
}


lambda_max <- max(t(scale(as.matrix(bc_trn[,-1]))) %*% bc_trn[,1]) / nrow(bc_trn[,-1])
lambda_list <- list(log(lambda_max), log(0.0001), -(log(lambda_max) - log(0.0001))/100, exp)

  lam_start <- lambda_list[[1]]
  lam_stop <- lambda_list[[2]]
  lam_step <- lambda_list[[3]]
  lam_func <- lambda_list[[4]]
  
init_lambda_vec <- lambda_init(lam_start, lam_stop, lam_step, lam_func)
```

\newpage 

## Appendix F: Cross Validation Implementation

```{r, eval=FALSE}

cv_jt <- function(k = 5, training, func, lam_start_stop_func, lambda_list){
  
  #### initializing lambda vector ####
  
  lam_start <- lambda_list[[1]]
  lam_stop <- lambda_list[[2]]
  lam_step <- lambda_list[[3]]
  lam_func <- lambda_list[[4]]
  
  lam_list <- tibble(
    lam_count = 0,
    lam_start = 0,
    lam_stop = 0,
    lam_step = 0
  )
  
  lam_count <- 0
  del_too_many_var <- 1
  out_res <- list()
  res <- list()
  
  while (del_too_many_var > 0) {

    lam_count <- lam_count + 1
    
    # saving lambda vector parameters
    cur_lam_list <- tibble(lam_count, lam_start, lam_stop, lam_step)
    
    lam_list <- bind_rows(lam_list, cur_lam_list) 
    
    new_lambda_vec <- lambda_init(lam_start, lam_stop, lam_step, lam_func)
    
    lasso_list <- list()
    auc_list <- list()
    
    for (i in 1:k) {
      
      # this will identify the training set as not i
      trn_set = 
        training %>% 
        filter(fold_id != i) %>% 
        select(-fold_id)
      
      # and this assigns i to be the test set
      tst_set =
        training %>% 
        filter(fold_id == i) %>% 
        select(-fold_id) %>%
        rename(y = diagnosis)
      
      # making matrices
      X_trn <- trn_set[,-1]
      Y_trn <- trn_set$diagnosis
      
      # lasso_list    
      lasso_list <- func(inputs = X_trn, output = Y_trn, lambda_vec = new_lambda_vec)
      
      lasso_lambda <- lasso_list[[1]]
      lasso_beta <- lasso_list[[2]]
      lasso_selected <- tibble(selected_num = lasso_list[[3]])
      
      lasso_lam_bet <- cbind(lasso_lambda, lasso_beta) %>% as.matrix()
      
      trn_roc <- auc_calc_lasso(lasso_lam_bet, tst_set)
      auc_list <- bind_rows(auc_list, trn_roc)
      
    }
    
    auc_res <- 
      auc_list %>% 
      group_by(lambda) %>%
      summarise(mean_auc = mean(auc_vals))
    
    res[[lam_count]] <- bind_cols(auc_res, lasso_selected)
    
    res[[lam_count]] <- res[[lam_count]] %>% 
      mutate(num_dropped_vars = selected_num - lag(selected_num, 1))
    
    del_too_many_var <- sum(na.omit(res$num_dropped_vars) > 1)
    
   if (del_too_many_var > 0) {
     #### performing grid search ####
    max_auc_lam <- res %>% filter(mean_auc == max(mean_auc)) %>% pull(lambda) %>% mean()
    lam_start <- lam_start_stop_func(max_auc_lam) + 2*abs(lam_step)
    lam_stop <- lam_start_stop_func(max_auc_lam) - 2*abs(lam_step)
    lam_step <- sign(lam_step)*(lam_start - lam_stop)/length(new_lambda_vec)
} 
  }
  
  # creating dataframe to show lambda values and corresponding mean AUC
  out_res[[1]] <- res
  out_res[[2]] <- lam_list
  out_res[[3]] <- auc_list
  
  return(out_res)
}
```

\newpage

## Appendix G: Grid Search

```{r, eval=FALSE}
# checking to see if more than one variable is selected between lambdas at any point
del_too_many_var <- sum(na.omit(res$num_dropped_vars) > 1)

# if more than one variable is selected/dropped, adjust range and step size
if (del_too_many_var > 0) {
    max_auc_lam <- res %>% filter(mean_auc == max(mean_auc)) %>% pull(lambda) %>% mean()
    lam_start <- lam_start_stop_func(max_auc_lam) + 2*abs(lam_step)
    lam_stop <- lam_start_stop_func(max_auc_lam) - 2*abs(lam_step)
    lam_step <- sign(lam_step)*(lam_start - lam_stop)/length(new_lambda_vec)
} 
```
