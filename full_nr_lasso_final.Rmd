---
title: "DUPLICATE: Full NR and LASSO Run (with standardized data)"
author: "Jimmy Kelliher"
date: "3/27/2022"
output: pdf_document
---

This file prepares our data, runs Newton-Raphson, runs LASSO, and plots some results at the bottom. I think this file should stay clean and relatively simple. I think the file should only take 15ish minutes to knit or run in full. I think the best idea is for plots to be saved as R objects and imported into the presentation / report, but open to ideas here.

```{r setup, warning=FALSE, message = FALSE}
source("./shared_code/setup.R")
knitr::opts_chunk$set(cache = TRUE)
```

```{r data import and partition, warning=FALSE, message = FALSE}
source("./shared_code/data_prep.R")
```

# Newton-Raphson

```{r newton-raphson function}
source("./shared_code/full_NR2.R")
source("./shared_code/roc_func.R")
roc_nr <- roc_func(nr_beta_est, bc_tst)
```

# LASSO

```{r external-functions, warning=FALSE, message=FALSE}
source("./shared_code/logistic_lasso.R")
source("./shared_code/auc_calc_lasso.R")
```

## Folding Data

```{r folding training data}
source("./shared_code/cv_folding.R")

bc_trn_folds <- 
  cv_sets(training = bc_trn) %>% 
  select(-fold_p)
```

```{r implementing cv}
source("./shared_code/cv_implementation.R")
lambda_max <- max(t(scale(as.matrix(bc_trn[,-1]))) %*% bc_trn[,1]) / nrow(bc_trn[,-1])
lambda_list <- list(log(lambda_max), log(0.0001), -(log(lambda_max) - log(0.0001))/100, exp)

#cv_res <- cv_jt(k = 5, training = bc_trn_folds, 
#                func = logistic_lasso, lam_start_stop_func = log, 
#                lambda_list = lambda_list)

# cv.glmnet(x, y, alpha = 1, family = "binomial", type.measure = "auc", lambda = exp(seq(log(lambda_max), log(0.00001), length.out = 1000)), nfolds = 5)
```

```{r lasso results}
# load data from final run
load("finalized_lasso_data.RData")

# pulling out key results
selected_lambda <- 
  cv_res[[1]][[length(cv_res[[1]])]] %>% 
  filter(mean_auc == max(mean_auc)) %>% 
  pull(lambda) %>% 
  min()

selected_lambda_minmax <- 
  cv_res[[3]] %>%
  group_by(lambda) %>%
  summarize(min_auc = min(auc_vals)) %>%
  ungroup() %>%
  filter(min_auc == max(min_auc)) %>%
  pull(lambda) %>%
  as.vector()
  


lasso_final_model <- logistic_lasso(inputs = bc_trn[,-1], 
                                    output = bc_trn[,1], 
                                    lambda_vec = selected_lambda)

lasso_minmax_model <- logistic_lasso(inputs = bc_trn[,-1],
                                     output = bc_trn[,1],
                                     lambda_vec = selected_lambda_minmax)

lasso_betas <- lasso_final_model[[2]] %>% t()
lasso_betas_minmax <- lasso_minmax_model[[2]] %>% t()
# this is just for viz purposes
tst_lambda_vec <- exp(seq(from = log(lambda_max), 
                          to = log(0.0001), 
                          by = -(log(lambda_max) - log(0.0001))/100))

#lasso_final_range <- logistic_lasso(inputs = bc_trn[,-1], 
#                                    output = bc_trn[,1], 
#                                    lambda_vec = tst_lambda_vec)

lfr_df <- data.frame(do.call(cbind, lasso_final_range)) %>% 
  select(-selected) %>% 
  pivot_longer(cols = starts_with("beta"),
               names_prefix = "beta.",
               names_to = "beta_coef",
               values_to = "coef_est")

# storing lasso ROC object
bc_tst[,-1] <- scale(bc_tst[,-1])
roc_lasso <- roc_func(lasso_betas, bc_tst)
roc_minmax <- roc_func(lasso_betas_minmax, bc_tst)

cv_res_lam <- cv_res[[1]]
```

```{r plotting_some_results}
# each of these plots could be saved as R objects and imported into other documents
beta_coef_plot <- 
  lfr_df %>% 
  group_by(lambda) %>% 
  filter(beta_coef != "intercept") %>% 
  ggplot(x = -log(lambda), y = coef_est, group = beta_coef) +
  geom_path(aes(x = -log(lambda), y = coef_est, group = beta_coef, col = beta_coef)) +
#  coord_cartesian(xlim = c(0, 0.05)) + # this zooms in on the plot, comment out if flipping
#  scale_x_reverse() +                  # this flips the x-axis if wanted
#  coord_cartesian(xlim = c(0.06, 0)) + # this zooms in if flipped
  geom_vline(xintercept = -log(selected_lambda), col = "black", linetype = "dashed") + 
  geom_text(aes(x = -log(selected_lambda), y = 5, label = "λ - Greatest Mean AUC"), colour = "black", size = 3, hjust = 1.05) +
  geom_vline(xintercept = -log(selected_lambda_minmax), col = "black", linetype = "dashed") +
  geom_text(aes(x = -log(selected_lambda_minmax), y = 5, label = "λ - Minimax AUC"), colour = "black", size = 3, hjust = 1.05) +
  labs(title = "Beta Coefficients in LASSO Model")

# this looks alright, might prefer a larger lambda range to show AUC down to 0.5
# but probably not important
auc_vs_lambda <- 
  cv_res[[1]][[1]] %>% 
  data.frame() %>% 
  ggplot(x = lambda, y = mean_auc) +
  geom_line(aes(x = lambda, y = mean_auc), col = "black") +
  geom_vline(xintercept = selected_lambda, linetype = "dashed", color = "red") +
  geom_point(aes(x = lambda, y = mean_auc), col = "black") +
  geom_text(aes(x = selected_lambda, y = 0.96, label = "λ - Greatest Mean AUC = 0.0101"), col = "red", size = 3, hjust = -0.05) +
#  coord_cartesian(xlim = c(0, 0.2), ylim = c(.9, 1)) +
  labs(title = "Mean AUC vs. Lambda",
       x = "Lambda",
       y = "Mean AUC")

save(cv_res, nr1, roc_lasso, roc_minmax, roc_nr, beta_coef_plot, betas_table_final, auc_vs_lambda, selected_lambda, selected_lambda_minmax, file = "full_final_res_tm.RData")

# Final ROC plots
auc_vec <- c(round(roc_lasso$auc[1], digits = 4),
             round(roc_minmax$auc[1], digits = 4),
             round(roc_nr$auc[1], digits = 4))
model_names <- c("LASSO Greatest Mean", "LASSO MinMax", "Newton-Raphson")
ggroc(list(roc_lasso, roc_minmax, roc_nr), legacy.axes = TRUE) +
  scale_color_discrete(labels = paste0(model_names, " (", auc_vec, ")"),
                       name = "Models (AUC)") +
  geom_abline(intercept = 0, slope = 1, color = "grey") +
  labs(title = "ROC Curves from Test Data Predictions")

## another option for plotting ROC curves
plot(roc_lasso, legacy.axes = TRUE, col = "goldenrod", 
     main = "ROC Curves from Test Data Predictions")
plot(roc_minmax, col = "darkgreen", add = TRUE)
plot(roc_nr, col = "dodgerblue", add = TRUE)
legend("bottomright", legend = paste0(model_names, ": ", auc_vec), 
       col = c("goldenrod", "darkgreen", "dodgerblue"), lwd = 2, bty = "n")
```

```{r tabling_some_results}
beta_names <- 
  (dimnames(lasso_betas)[1]) %>% 
  data.frame() %>% 
  rename(beta_coef = 1)

betas_table_final <- 
  cbind(nr_beta_est, lasso_betas, lasso_betas_minmax) %>% 
  data.frame() %>% 
  round(digits = 4) %>% 
  rename(NewtonRaphson = 1, LASSO_GreatestMean = 2, LASSO_MinMax = 3) %>% 
  knitr::kable(caption = "Final Beta Coefficient Estimates")

kableExtra::save_kable(x = betas_table_final, file = "beta_table.pdf")
```

## Comparison to GLMNET Lasso

```{r}
# standardize bc_trn[,-1]
bc_cov <- as.matrix(scale(bc_trn[,-1]))

glmnet_fit <- glmnet(x = bc_cov , y = bc_trn[,1], family = "binomial", lambda = selected_lambda)
glmnet_est <- as.vector(coef(glmnet_fit))

comp_est <- tibble(
  names = lasso_betas %>% rownames(),
  glmnet_est = glmnet_est,
  lasso_est = lasso_betas,
  diff = glmnet_est - lasso_est
)
comp_est %>% knitr::kable()
```

The GLMNET lasso regression procedure and our logistic lasso procedure produce very similar (near identical) results. Yay!























