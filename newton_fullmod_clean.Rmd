---
title: "Newton Raphson Full Model"
author: "Waveley Qiu (wq2162)"
date: "2022-03-22"
output: pdf_document
---

```{r setup, include=FALSE}
source("shared_code/setup.R")
source("shared_code/partition.R")

library(tidyverse)
library(pROC)
library(kableExtra)
library(knitr)
library(xtable)
library(glmnet)
```

## EDA

Let's import and take a look at the data.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
bc <- read.csv("data/breast-cancer.csv") %>% as_tibble()
```

Let's take a look at the distributions of other variables.
```{r, echo=FALSE, warning=FALSE, message=FALSE}
bc %>% 
  select(diagnosis, contains("mean")) %>%
  gather(key = "stat", value = "value", 2:11) %>% 
  ggplot(aes(x = value, fill = diagnosis)) +
    facet_wrap(~ stat, scales = "free") +
    geom_density(alpha = 0.5)
```

```{r}
tbl_summary(bc, by = diagnosis)
```

We want our outcome variable to be binary. Let's create a new outcome variable, `y`, which will be 1 if `diagnosis == 'M'` and 0 if `diagnosis == 'B'.

```{r}
bc <- bc %>% mutate(y = ifelse(diagnosis == "M", 1, 0)) %>% relocate(y)
```

## Multicollinearity Investigation

Let's look at the correlations for all predictors.

```{r, fig.width = 20, fig.asp=0.8}
cor_cov_all <- cor(bc %>% select(-c(id, y, diagnosis)))

corrplot::corrplot(cor_cov_all, 
                   method = "color",
                   type = "lower", 
                   insig = "blank",
                   diag = FALSE)
```

Yikes. Let's make some cuts ($r^2 > 0.9$) and reevaluate. To begin, we zoom in on our culprits.

```{r}
# call correlation matrix
corr_df <- cor(bc %>% select(-c(id, y, diagnosis)))

# convert matrix to tidy data frame and restrict to highly correlated pairs
corr_df <-
  data.frame(
    v1   = rownames(corr_df)[row(corr_df)[upper.tri(corr_df)]]
  , v2   = colnames(corr_df)[col(corr_df)[upper.tri(corr_df)]]
  , corr = corr_df[upper.tri(corr_df)]
  ) %>%
  arrange(-abs(corr)) %>%
  filter(abs(corr) > 0.90)

# output results
corr_df %>% knitr::kable()
```

The variables {`area_mean`, `area_worst`, `perimeter_mean`, `perimeter_worst`, `radius_mean`, `radius_worst`} are all mutually correlated. Mathematically, if we consider the equivalence classes of variables that are highly correlated, these six variables would belong to the same equivalence class. Thus, it would be great if we could identify the variable that is the best collective proxy for the other members of this class.

```{r}
# identify largest equivalence class of proxies
corr_df_sub1 <- corr_df %>% 
  filter(
      v1 == "area_mean"       | v2 == "area_mean"
    | v1 == "area_worst"      | v2 == "area_worst"
    | v1 == "perimeter_mean"  | v2 == "perimeter_mean"
    | v1 == "perimeter_worst" | v2 == "perimeter_worst"
    | v1 == "radius_mean"     | v2 == "radius_mean"
    | v1 == "radius_worst"    | v2 == "radius_worst"
  )

# identify variable that is the best proxy for the other ones
rbind(corr_df_sub1[ , -2]  %>% rename(v = v1), corr_df_sub1[ , -1] %>% rename(v = v2)) %>%
  group_by(v) %>%
  summarize(
      mean_corr = mean(corr)
    , std_corr  = sd(corr)
    , min_corr  = min(corr)
  ) %>%
  arrange(-mean_corr) %>%
  knitr::kable()
```

The variable `radius_worst` is the best collective proxy for the other five variables in its collective equivalence class, so we will retain `radius_worst` and discard the rest. Let's look at the remaining variables.

```{r}
corr_df %>% 
  filter(!(
      v1 == "area_mean"       | v2 == "area_mean"
    | v1 == "area_worst"      | v2 == "area_worst"
    | v1 == "perimeter_mean"  | v2 == "perimeter_mean"
    | v1 == "perimeter_worst" | v2 == "perimeter_worst"
    | v1 == "radius_mean"     | v2 == "radius_mean"
    | v1 == "radius_worst"    | v2 == "radius_worst"
  )) %>%
  knitr::kable()
```

Again, in the spirit of identifying the best representative of each equivalence class, we keep `radius_se`, `concave.points_worst`, and `texture_mean`. In total, we have discarded ten variables.

```{r, fig.width = 20, fig.asp=0.8}
bad_vars <- c(
    "area_mean", "area_worst", "perimeter_mean", "perimeter_worst", "radius_mean"
  , "perimeter_se", "area_se"
  , "concave.points_worst", "concavity_mean"
  , "texture_worst"
)

covariates <-
  bc %>% 
  select(-c(id, y, diagnosis)) %>%
  select(-bad_vars)

cor_cov <- cor(covariates)


corrplot::corrplot(cor_cov, 
                   method = "color",
                   type = "lower", 
                   insig = "blank",
                   diag = FALSE)
```

Better. We'll proceed with these predictors.

```{r}
fin_names <- covariates %>% names()
```


## Full Model

We want to establish logistic model using all variables in the dataset. We will do this by performing a Newton Raphson optimization in order to find the MLEs of the beta coefficients.

The likelihood function for a logistic model is defined as follows:

$$
f(\beta_0, \beta_1,...,\beta_{p}) = \sum_{i=1}^n \left(Y_i\left(\beta_0 + \sum_{j=1}^{p} \beta_jx_{ij}\right) - \log(1+e^{\left(\beta_0 + \sum_{j=1}^{p} \beta_jx_{ij}\right)}\right)
$$

Let $\pi_i = \frac{e^{\beta_0 + \sum_{j=1}^{p} \beta_jx_{ij}}}{1+e^{\beta_0 + \sum_{j=1}^{p} \beta_jx_{ij}}}$. Then, the gradient of this function is defined as follows:

$$
\triangledown f(\beta_0, \beta_1, ..., \beta_{p}) = 
\left(
\begin{matrix}
\sum_{i=1}^n Y_i-\pi_i \\ 
\sum_{i=1}^n x_{i1}(Y_i - \pi_i)\\
\sum_{i=1}^n x_{i2}(Y_i - \pi_i)\\
\vdots \\
\sum_{i=1}^n x_{ip}(Y_i - \pi_i)
\end{matrix}
\right)
$$
Finally, we define the Hessian of this function as follows:

$$
\begin{aligned}
\triangledown^2f(\beta_0, \beta_1, ..., \beta_{p}) &=  -\sum_{i=1}^n \left(
\begin{matrix}
1 \\ x_{i1} \\ x_{i2} \\ \vdots \\ x_{ip}
\end{matrix}
\right) \left(1 \,\, x_{i1} \,\, x_{i2} \,\, \dots \,\, x_{ip}\right)\pi_i(1-\pi_i)
\\
&= -\left(
\begin{matrix}
\sum_{i=1}^n \pi_i(1-\pi_i) &\quad \sum_{i=1}^n x_{i1}\pi_i(1-\pi_i) &\quad \dots &\quad \sum_{i=1}^nx_{ip}\pi_i(1-\pi_i) \\
\sum_{i=1}^n x_{i1}\pi_i(1-\pi_i) &\quad\sum_{i=1}^n x_{i1}^2\pi_i(1-\pi_i) &\quad \dots &\quad \sum_{i=1}^n x_{ip}x_{i1}\pi_i(1-\pi_i) \\
\sum_{i=1}^n x_{i2}\pi_i(1-\pi_i) &\quad\sum_{i=1}^n x_{i1}x_{i2}\pi_i(1-\pi_i) &\quad \dots &\quad \sum_{i=1}^n x_{ip}x_{i2}\pi_i(1-\pi_i) \\
\vdots &\quad \ddots &\quad \ddots &\quad \vdots \\
\sum_{i=1}^n x_{ip}\pi_i(1-\pi_i) &\quad\sum_{i=1}^n x_{i1}x_{ip}\pi_i(1-\pi_i) &\quad \dots &\quad \sum_{i=1}^n x_{ip}^2\pi_i(1-\pi_i) 
\end{matrix}
\right)
\\
&= \left(1 \,\, x_{i1} \,\, x_{i2} \,\, \dots \,\, x_{ip}\right) I(\pi_i(1-\pi_i))\left(
\begin{matrix}
1 \\ x_{i1} \\ x_{i2} \\ \vdots \\ x_{ip}
\end{matrix}
\right)
\end{aligned}
$$

Let's create a function that produces the log-likelihood, gradient vector, and hessian matrix, given a dataset and beta vector:

```{r, warning=FALSE, messsage=FALSE}
loglike_func <- function(dat, betavec){
  
  dat = bc_train
  
  # x matrix
  dat_temp <-
    dat %>%
    mutate(intercept = 1) %>%
    select(-y) %>%
    relocate(intercept)
  
  dat_x <-  
    dat_temp %>%
    as.matrix() %>%
    unname() 
  
  # pi vector
  u <- dat_x %*% betavec
  pi <- exp(u) / (1 + exp(u))
  
  # loglikelihood
  loglik <- sum(dat$y*u - log(1 + exp(u)))
  
  #gradient 
  grad <- t(dat_x) %*% (dat$y - pi)
  
  # Hessian 
  W <- diag(nrow(pi))
  diag(W) <- pi*(1 - pi)
  hess <- -(t(dat_x) %*% W %*% (dat_x))
  
  return(list(loglik = loglik, grad = grad, hess = hess))
  
}
```


Now, let's construct a Newton Raphson algorithm to determine $\beta_i$ coefficients that maximize the likelihood of the function.

```{r, warning=FALSE, message=FALSE}

NewtonRaphson <- function(dat,  start, tol = 1e-8, maxiter = 200){
  
  i <- 0
  cur <- start 
  stuff <- loglike_func(dat, cur)
  res <- c(i = 0, "loglik" = stuff$loglik,  "step" = 1, cur)
  
  prevloglik <- -Inf # to make sure it iterates
  
  while (i < maxiter && abs(stuff$loglik - prevloglik) > tol) {
    step <- 1
    i <- i + 1
    prevloglik <- stuff$loglik
    
    # check negative definite  
    eigen_vals <- eigen(stuff$hess)
    
    if (max(eigen_vals$values) <= 0 ) { # check neg def, if not change 
      hess <- stuff$hess
    } else { # if it is pos def then need to adjust 
      hess <- stuff$hess - (max(eigen_vals$values) + 0.1)*diag(nrow(stuff$hess))
    } 
    
    prev  <- cur
    cur   <- prev - step*(solve(stuff$hess) %*% stuff$grad)
    stuff <- loglike_func(dat, cur) # log-lik, gradient, Hessian
    
    # step halving
    while (stuff$loglik < prevloglik) {
      stuff <- loglike_func(dat, prev)
      step  <- step / 2 # this is where half steping happens 
      cur   <- prev - step*(solve(stuff$hess) %*% stuff$grad)
      stuff <- loglike_func(dat, cur)
    }
    # add current values to results matrix
    res <- rbind(res, c(i, stuff$loglik, step, cur))
  }
  
  colnames(res) <- c("i", "loglik",  "step", "intercept", names(dat[,-1]))
  return(res)
}
```

## Splitting Testing and Training Data

Using a function from `partition.R`, we will split our data into testing and training. We will include only the variables we have identified as being sufficiently uncorrelated.

```{r}
col.num <- which(colnames(bc) %in% fin_names) # get those variables 
bc_trunc <- bc[, c(1, col.num)]

bc_test_train <- partition(0.8, bc_trunc)

bc_test <- bc_test_train %>% filter(part_id == "test") %>% select(-part_id)
bc_train <- bc_test_train %>% filter(part_id == "train") %>% select(-part_id)
```

## Running Newton-Raphson Algorithm

Let's establish a beta vector and run the Newton-Raphson algorithm we've established on the training data.

```{r}
betavec <- c(rep(0.01, ncol(bc_train))) %>% as.matrix()
nr1 <- NewtonRaphson(bc_train, betavec)
beta_est <- nr1[nrow(nr1), -c(1:3)] %>% as.vector()
```

## Compare with GLM and GLMNET

Let's run GLM.

```{r}
glm_fit <- glm(y~., data = bc_train, family = "binomial")
result_glm <- summary(glm_fit)

glm_est <- glm_fit %>% 
  broom::tidy() %>% 
  select(term, estimate) %>% 
  mutate(glm_est = round(estimate, 3)) %>%
  select(-estimate) %>% 
  mutate(term = ifelse(term == "(Intercept)", "intercept", term)) %>% pull(glm_est)
```

Now, let's run GLMNET.

```{r}
xdat <- bc_train %>% select(-y) %>% as.matrix()
glmnet_fit <- glmnet(x = xdat, y = bc_train$y, family = "binomial", lambda = 0, thresh = 10^-12)
glmnet_est <- coef(glmnet_fit) %>% as.vector() %>% round(3)
```


```{r}
tibble(
  beta_sub = 0:(length(beta_est) - 1),
  nr_est = round(beta_est, 3),
  glm_est = glm_est,
  glmnet_est = glmnet_est
) %>%
  knitr::kable()
```


## AUC

```{r}
auc_calc_full <- function(beta_est, test_data){
  # pulling out the terms used in the full model (should be all)
  
  # we have this flexible in case we want to test fewer variables 
  # terms <- beta_est %>% pull(term) 
  # col.num <- which(colnames(test_data) %in% terms)
  # select the desired x values 
  
  xvals <- test_data[,-1] %>% 
     mutate(
      intercept = 1 # create a intercept variable 
    ) %>%
    relocate(intercept) # move it to the front 
  
  pred <- as.matrix(xvals) %*% beta_est # get the cross product of the linear model
  logit_pred <- exp(pred) / (1 + exp(pred)) # link function to get probabilities
  
  auc_val <- auc(test_data$y, as.vector(logit_pred)) # calculating the AUC
  
  # roc(tst_data$y, as.vector(logit_pred)) %>% plot( legacy.axes=TRUE) # graphs AUC
  return(auc_val)

}

auc <- auc_calc_full(beta_est, bc_test)

```


The AUC of the Newton-Raphson Model is `r auc`.


## Function for Newton Rapshon Coefficeint Values Change plot

```{r, fig.width=12, , fig.height=15}
coeff_change_plot <- function(result_df, glm_fit){
  nr1 <- data.frame(nr1)
  nr1 <- nr1[-1,] %>% select(-i, -loglik, -step)


  nr1 <- 
    nr1 %>% mutate(i = paste0("Iteration", 1:nrow(nr1))) %>% 
    relocate(i) %>% `rownames<-`(NULL)
  
  solution <-
  data.frame(t(glm_fit$coefficients)) %>% 
  janitor::clean_names() %>%
  rename(intercept = x_intercept) %>%
  mutate(i = "Solution")
  
  coeff_change_plot <-
    nr1 %>%
    janitor::clean_names() %>%
    bind_rows(solution) %>%
    pivot_longer(intercept:fractal_dimension_worst,
                 names_to = "Predictors",
                 values_to = "Coefficient_Values") %>%
    mutate(i = fct_relevel(i, "Iteration1","Iteration2","Iteration3","Iteration4",
                           "Iteration5","Iteration6","Iteration7","Iteration8",
                           "Iteration9","Iteration10","Iteration11","Iteration12",
                           "Iteration13","Iteration14")) %>%
    ggplot(aes(x = i, y = Coefficient_Values, group = Predictors, color = Predictors)) +
    geom_line() + 
    theme(legend.position = "bottom", plot.title = element_text(hjust = 0.5, face = "bold")) +
    labs(y = "Coefficient Values",
         title = "Newton Rapshon Coefficeint Values Change over Iteraion")
  
  return(coeff_change_plot)
}

coeff_change_plot(nr1, glm_fit)
```

## Function for Newton Rapshon Coefficeint Convergence plot

```{r, fig.width=12, , fig.height=15}
coeff_convergence_plot <- function(result_df, glm_fit){
  nr1 <- data.frame(nr1)
  nr1 <- nr1[-1,] %>% select(-i, -loglik, -step)


  nr1 <- 
    nr1 %>% mutate(i = paste0("Iteration", 1:nrow(nr1))) %>% 
    relocate(i) %>% `rownames<-`(NULL)
  
  solution <-
    data.frame(t(glm_fit$coefficients)) %>% 
    janitor::clean_names() %>%
    rename(intercept = x_intercept)  %>% 
    pivot_longer(intercept:fractal_dimension_worst, 
                 names_to = "Predictors", values_to = "Solution")
  
  coeff_convergence_plot <-
    nr1 %>%
    janitor::clean_names() %>%
    pivot_longer(intercept:fractal_dimension_worst,
                 names_to = "Predictors", 
                 values_to = "Coefficient_Values") %>%
    left_join(solution, by = "Predictors") %>%
    mutate(i = fct_relevel(i, "Iteration1","Iteration2","Iteration3","Iteration4",
                           "Iteration5","Iteration6","Iteration7","Iteration8",
                           "Iteration9","Iteration10","Iteration11","Iteration12",
                           "Iteration13","Iteration14")) %>%
    mutate(Difference = Coefficient_Values - Solution) %>%
    ggplot(aes(x = i, y = Difference, group = Predictors, color = Predictors)) +
    geom_line() +
    geom_point(aes(x = 14.05, y = 0), col = "lightblue") +
    theme(legend.position = "bottom", plot.title = element_text(hjust = 0.5, face = "bold")) +
    labs(y = "Difference between coefficient values and solution",
         title = "Newton Rapshon Coefficeint Convergence over Iteration") 
  
  return(coeff_convergence_plot)
}

coeff_convergence_plot(nr1, glm_fit)
```


```{r, fig.width=12, , fig.height=15}
sigmoid_plot <- function(result_df, training_data, glm_fit){
  nr1 <- data.frame(nr1)
  nr1 <- nr1[-1,] %>% select(-i, -loglik, -step)
  
  X <- as.matrix(bc_train)
  v1 <- unlist(nr1[1,], use.name = FALSE) %>% as.matrix()
  log_odd <- X %*% v1
  Probability <- 1/(1+exp(-log_odd))
  
  plot(log_odd,Probability, col = "black", cex=1.3, 
       xlab = "Log[p(1-p)]", main = "Newton Rapshon Logistic Fit over Iteration" )
  
  X <- as.matrix(bc_train)
  z <- list()
  p <- list()
  
  library(RColorBrewer)
  mycolors = c(brewer.pal(name="Dark2", n = 8), brewer.pal(name="Paired", n = 6))
  
  for (i in 2:nrow(nr1)) {
    v = unlist(nr1[i,], use.name = FALSE) %>% as.matrix()
    z[[i]] = X %*% v
    p[[i]] = 1/(1+exp(-z[[i]]))
    
    par(new=TRUE)
    plot(z[[i]],p[[i]], col = mycolors[i], xaxt="n", yaxt="n", xlab="", ylab="", cex=1.3)
  }
  legend(x = "bottomright",
          legend = c(paste(rep(c("Iteration")), 1:14), "Solution")
                         ,col=c("black",mycolors[2:14], "blue"),lwd=1,
                         bty="n",ncol=3,cex=1.5,pt.cex=1.5,xpd=TRUE)
  solution <-
  data.frame(t(glm_fit$coefficients))
  
  par(new=TRUE)
  v_sol <- unlist(solution, use.name = FALSE) %>% as.matrix()
  Fit_sol <- X %*% v_sol
  p_sol <- 1/(1+exp(-Fit_sol))
  
  plot(Fit_sol, p_sol, col = rgb(red = 0, green = 0, blue = 1, alpha = 0.5), 
       xaxt="n", yaxt="n", xlab="", ylab="", pch = 20)
    
}

sigmoid_plot(nr1, bc_train, glm_fit)
```



